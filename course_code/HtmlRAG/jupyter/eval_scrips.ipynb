{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0e97acd18cef7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "import json\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from evaluate_utils import calculate_short_answer_EM, rouge, bleu, select_candidate\n",
    "\n",
    "version=\"v0915\"\n",
    "multi_docs=\"top10\"\n",
    "src_granularity=256\n",
    "granularity=128\n",
    "\n",
    "def eval_short_answer_EM(dataset, chat_model, reference_format, split, search_engine, rewrite_method):\n",
    "    output_dir = f\"../html_data/{dataset}/{chat_model}/{search_engine}\"\n",
    "\n",
    "    if dataset in [\"asqa\", \"nq\", \"eli5\"]:\n",
    "        #. fine trim ratio 2/3\n",
    "        coarse_context_window = {\"2k\": \"3k\", \"4k\": \"6k\", \"8k\": \"12k\", \"16k\": \"24k\", \"128k\": \"192k\"}[context_window]\n",
    "    else:\n",
    "        #. fine trim ratio 1/2\n",
    "        coarse_context_window = {\"2k\": \"4k\", \"4k\": \"8k\", \"8k\": \"16k\", \"16k\": \"32k\", \"128k\": \"192k\"}[context_window]\n",
    "    if reference_format in [\"html-trim\", \"fill-chunk\"]:\n",
    "        output_file = f\"{output_dir}/{chat_model}-{reference_format}-{rewrite_method}-{rerank_model}-{dataset}-{split}.jsonl\"\n",
    "    elif reference_format == \"tree-gen\":\n",
    "        output_file = f\"{output_dir}/{chat_model}-{reference_format}-{rewrite_method}-{version}-{granularity}-{dataset}-{split}.jsonl\"\n",
    "    elif reference_format == \"tree-rerank\":\n",
    "        output_file = f\"{output_dir}/{chat_model}-{reference_format}-{rewrite_method}-{rerank_model}-{granularity}-{dataset}-{split}.jsonl\"\n",
    "    elif reference_format in [\"chunk-rerank-tree-gen\", \"tree-rerank-tree-gen\"]:\n",
    "        output_file = f\"{output_dir}/{chat_model}-{reference_format}-{rewrite_method}-{rerank_model}-{src_granularity}to{granularity}-{coarse_context_window}-{version}-{dataset}-{split}.jsonl\"\n",
    "    elif reference_format in [\"llmlingua\", \"bgelargeen\",\"e5-mistral\"]:\n",
    "        output_file = f\"{output_dir}/{chat_model}-{reference_format}-{rewrite_method}-{dataset}-{split}.jsonl\"\n",
    "    else:\n",
    "        output_file = f\"{output_dir}/{chat_model}-{reference_format}-{rewrite_method}-{dataset}-{split}.jsonl\"\n",
    "    print(f\"evaluating file {output_file}\")\n",
    "    \n",
    "    try:\n",
    "        data_lines = [json.loads(l) for l in open(output_file)]\n",
    "        generated_answers = [data_line[f\"{chat_model}_{reference_format}\" ] for data_line in data_lines]\n",
    "        if dataset == \"eli5\":\n",
    "            #. eval long answer\n",
    "            if \"answer\" in data_lines[0]:\n",
    "                gold_answers=[data_line[\"answer\"] for data_line in data_lines]\n",
    "            else:\n",
    "                gold_answers=[data_line[\"long_answers\"] for data_line in data_lines]\n",
    "                \n",
    "            selected_gold_answers=[]\n",
    "            for gen, gold in tqdm.tqdm(zip(generated_answers, gold_answers), total=len(generated_answers)):\n",
    "                selected_gold_answers.append(select_candidate(gen, gold))\n",
    "            rouge_result=rouge.compute(predictions=generated_answers, references=selected_gold_answers)\n",
    "            rouge_result={k: round(v * 100, 2) for k, v in rouge_result.items()}\n",
    "            \n",
    "            bleu_result=bleu.compute(predictions=generated_answers, references=gold_answers)\n",
    "            return {**rouge_result, **bleu_result}\n",
    "        \n",
    "        if \"answers\" in data_lines[0]:\n",
    "            answers = [data_line['answers'] for data_line in data_lines]\n",
    "        elif \"short_answers\" in data_lines[0]:\n",
    "            answers = [data_line['short_answers'] for data_line in data_lines]\n",
    "        elif \"answer\" in data_lines[0]:\n",
    "            answers = [data_line['answer'] for data_line in data_lines]\n",
    "        else:\n",
    "            raise NotImplementedError(\"answers not found in data_lines\")\n",
    "    \n",
    "        \n",
    "        exact_match = [calculate_short_answer_EM(generated_answer, gold_answers) for generated_answer, gold_answers in zip(generated_answers, answers)]\n",
    "        hit1= round(sum([hit1[\"hit1\"] for hit1 in exact_match])/len(exact_match)*100, 2),\n",
    "        exact_match= round(sum([hit1[\"exact_match\"] for hit1 in exact_match])/len(exact_match)*100, 2)\n",
    "        print(f\"chat_model: {chat_model}, reference_format: {reference_format}, dataset: {dataset}, split: {split}, hit1: {hit1}, exact_match: {exact_match}\")\n",
    "        return {\n",
    "            \"hit1\": hit1,\n",
    "            \"exact_match\": exact_match\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"error evaluating file {output_file}, error: {e}\")\n",
    "        #  print stack trace\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        if dataset == \"eli5\":\n",
    "            return {\n",
    "                \"rouge1\": .0,\n",
    "                \"rouge2\": .0,\n",
    "                \"rougeL\": .0,\n",
    "                \"bleu\": .0,\n",
    "            }\n",
    "        return {\n",
    "            \"exact_match\": .0,\n",
    "            \"hit1\": .0,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tqdm\n",
    "#  eval all datasets\n",
    "#  generate latex table report\n",
    "context_window=\"4k\"\n",
    "reference_formats=[\"bm25\", \"bgelargeen\", \"e5-mistral\", \"llmlingua\", \"jinaai-reader\", \"tree-rerank-tree-gen\"]\n",
    "syn_names=[\"BM25\", \"BGE\", \"E5-Mistral\", \"LongLLMLingua\", \"JinaAI Reader\", \"HTML4RAG\"]\n",
    "\n",
    "# long context settings\n",
    "# context_window=\"128k\"\n",
    "# reference_formats=[\"html\", \"raw-text\", \"markdown\", \"html-simple\"]\n",
    "# syn_names=[\"Vanilla HTML\", \"Raw Text\", \"Markdown\", \"HTML4RAG-Clean\"]\n",
    "\n",
    "datasets=[\"asqa\", \"hotpot-qa\", \"nq\", \"trivia-qa\", \"musique\", \"eli5\"]\n",
    "\n",
    "split=\"test\"\n",
    "search_engine=\"bing\"\n",
    "rewrite_method=\"slimplmqr\"\n",
    "rerank_model=\"bgelargeen\"\n",
    "\n",
    "import multiprocessing\n",
    "res_list=multiprocessing.Manager().list([\"\"]*len(datasets)*len(reference_formats))\n",
    "processes = []\n",
    "\n",
    "def append_res2markdown_table(lidx, *args, **kwargs):\n",
    "    lidx=lidx\n",
    "    res=eval_short_answer_EM(*args)\n",
    "    if \"hit1\" in res:\n",
    "        hit1, exact_match=res[\"hit1\"], res[\"exact_match\"]\n",
    "        if isinstance(hit1, tuple):\n",
    "            hit1=hit1[0]\n",
    "        if isinstance(exact_match, tuple):\n",
    "            exact_match=exact_match[0]\n",
    "        if args[0] in [\"hotpot-qa\", \"musique\"]:\n",
    "            res=f\" {hit1} \"\n",
    "        else:\n",
    "            res=f\" {hit1} & {exact_match} \"\n",
    "    \n",
    "        res_list[lidx]=res\n",
    "    elif \"rouge1\" in res:\n",
    "        res=\" & \".join([f\"{v:.2f}\" for k, v in res.items()])\n",
    "        res_list[lidx]=res\n",
    "    else:\n",
    "        bleu=res[\"bleu\"] *100\n",
    "        res=f\" {bleu:.2f} \"\n",
    "        res_list[lidx]=res\n",
    "\n",
    "pbar=tqdm.tqdm(total=len(datasets)*len(reference_formats))\n",
    "\n",
    "chat_model=f\"llama70b{context_window}\"\n",
    "# chat_model=f\"llama8b{context_window}\"\n",
    "\n",
    "for i, dataset in enumerate(datasets):\n",
    "    for j, reference_format in enumerate(reference_formats):\n",
    "        lidx= i*len(reference_formats) + j\n",
    "        p=multiprocessing.Process(target=append_res2markdown_table, args=(lidx, dataset, chat_model, reference_format, split, search_engine, rewrite_method))\n",
    "        processes.append(p)\n",
    "        p.start()\n",
    "        pbar.update(1)\n",
    "        if len(processes) >= 4:\n",
    "            for p in processes:\n",
    "                p.join()\n",
    "            processes=[]\n",
    "                \n",
    "if processes:\n",
    "    for p in processes:\n",
    "        p.join()\n",
    "\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29ef20145d0c603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a latex table\n",
    "import re\n",
    "latex_table = [\"Dataset & EM & Hit@1 & EM & EM & Hit@1 & EM & Hit@1 & EM & ROUGE-L & BLEU\"]\n",
    "\n",
    "longest_syn_name = max([len(syn_name) for syn_name in syn_names]) +2\n",
    "for i in range(len(reference_formats)):\n",
    "    latex_table.append(f\"{syn_names[i]}\"+\" \"*(longest_syn_name-len(syn_names[i])) + \"&\")\n",
    "    \n",
    "for i, dataset in enumerate(datasets):\n",
    "    for j, reference_format in enumerate(reference_formats):\n",
    "        lidx= i*len(reference_formats) + j\n",
    "        latex_table[j+1] += f\"{res_list[lidx]} &\"\n",
    "        #. replace .x with .x0, e.g. 5.5 with 5.50\n",
    "        latex_table[j+1]=re.sub(r\"(\\d+\\.\\d)(?!\\d)\", r\"\\g<1>0\", latex_table[j+1])\n",
    "        \n",
    "\n",
    "for line in latex_table:\n",
    "    if line.endswith(\"&\"):\n",
    "        line=line[:-1]\n",
    "    line += \"\\\\\\\\\"\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd148cc21ba00f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#. create a markdown table\n",
    "markdown_table = [\"| Dataset | EM | Hit@1 | EM | EM | Hit@1 | EM | Hit@1 | EM | ROUGE-L | BLEU |\",\n",
    "                  \"| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\"]\n",
    "\n",
    "longest_syn_name = max([len(syn_name) for syn_name in syn_names]) +2\n",
    "for i in range(len(reference_formats)):\n",
    "    markdown_table.append(f\"| {syn_names[i]}\"+\" \"*(longest_syn_name-len(syn_names[i])) + \"|\")\n",
    "    \n",
    "for i, dataset in enumerate(datasets):\n",
    "    for j, reference_format in enumerate(reference_formats):\n",
    "        lidx= i*len(reference_formats) + j\n",
    "        markdown_table[j+1] += f\"{res_list[lidx]} |\"\n",
    "        #. replace .x with .x0, e.g. 5.5 with 5.50\n",
    "        markdown_table[j+1]=re.sub(r\"(\\d+\\.\\d)(?!\\d)\", r\"\\g<1>0\", markdown_table[j+1])\n",
    "        \n",
    "for line in markdown_table:\n",
    "    print(line)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
